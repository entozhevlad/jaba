version: "3.9"

services:
  arkasha:
    build: .
    image: arkasha-api
    ports:
      - "8080:8080"
    volumes:
      - ./data:/app/data

  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    environment:
      - JAVA_TOOL_OPTIONS=-Duser.name=spark -Duser.home=/tmp
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true
      - HADOOP_USER_NAME=root
      - SPARK_CONF_DIR=/app/conf
      - HOME=/tmp
      - SPARK_LOCAL_DIRS=/tmp
      - SPARK_USER=root
    ports:
      - "7077:7077"
      - "8081:8080" # UI
    volumes:
      - .:/app
    working_dir: /app
    entrypoint: ["/bin/bash", "-c",
                 "mkdir -p /tmp/.ivy2 /tmp/ivy && /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master"
    ]

  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - JAVA_TOOL_OPTIONS=-Duser.name=spark -Duser.home=/tmp
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - HADOOP_USER_NAME=root
      - SPARK_CONF_DIR=/app/conf
      - HOME=/tmp
      - SPARK_LOCAL_DIRS=/tmp
      - SPARK_USER=root
    ports:
      - "8082:8081"
    volumes:
      - .:/app
    working_dir: /app
    entrypoint: ["/bin/bash", "-c",
                 "mkdir -p /tmp/.ivy2 /tmp/ivy && /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
    ]

  spark-lab2:
    image: bitnami/spark:3.5
    container_name: spark-lab2
    depends_on:
      - spark-master
      - spark-worker
    working_dir: /app
    user: "0:0"
    volumes:
      - .:/app
      - ./conf:/app/conf
      - ./spark-warehouse:/app/spark-warehouse
    environment:
      - HOME=/tmp
      - SPARK_CONF_DIR=/app/conf
      - USER=spark
      - SPARK_USER=spark
      - HADOOP_USER_NAME=spark
      - JAVA_TOOL_OPTIONS=-Duser.name=spark -Dhadoop.user.name=spark
    entrypoint:
      [
        "/opt/bitnami/spark/bin/spark-submit",
        "--master", "local[*]",
        "--class", "com.example.kvdb.spark.Lab2Job",

        "--conf", "spark.driver.host=spark-lab2",
        "--conf", "spark.driver.bindAddress=0.0.0.0",
        "--conf", "spark.jars.ivy=/tmp/ivy",

        "--conf", "spark.hadoop.security.authentication=simple",
        "--conf", "spark.hadoop.security.authorization=false",
        "--conf", "spark.hadoop.fs.defaultFS=file:///",
        "--conf", "spark.hadoop.fs.file.impl=org.apache.hadoop.fs.LocalFileSystem",
        "--conf", "spark.hadoop.fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs",

        "--conf", "spark.sql.warehouse.dir=/app/spark-warehouse",

        "--conf", "spark.driver.extraJavaOptions=-Duser.name=spark -Dhadoop.user.name=spark",
        "--conf", "spark.executor.extraJavaOptions=-Duser.name=spark -Dhadoop.user.name=spark",
        "--conf", "spark.executorEnv.USER=spark",
        "--conf", "spark.executorEnv.HADOOP_USER_NAME=spark",

        "/app/target/arkasha-1.0-SNAPSHOT-jar-with-dependencies.jar"
      ]
    ports:
      - "4040:4040"
    restart: "no"

